{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!cd gym && pip install -e.[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-19 19:31:27,240] Making new env: FrozenLakeNotSlippery-v0\n"
     ]
    }
   ],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<FrozenLakeEnv<FrozenLakeNotSlippery-v0>>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FLNotSlippery_set_observation(env, observation_index):  # Would be cleaner to split it into rows and columns and use loops...\n",
    "    env.reset()\n",
    "    \n",
    "    if observation_index == 1:\n",
    "        env.step(2)\n",
    "    if observation_index == 2:\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "    if observation_index == 3:\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "    if observation_index == 4:\n",
    "        env.step(1)\n",
    "    if observation_index == 5:\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "    if observation_index == 6:\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "        env.step(1)\n",
    "    if observation_index == 7:\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "        env.step(1)\n",
    "    if observation_index == 8:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "    if observation_index == 9:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "    if observation_index == 10:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "    if observation_index == 11:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "        env.step(2)\n",
    "    if observation_index == 12:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "    if observation_index == 13:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "        env.step(1)\n",
    "    if observation_index == 14:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "    if observation_index == 15:\n",
    "        env.step(1)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "        env.step(1)\n",
    "        env.step(2)\n",
    "        observation, _, _, _ = env.step(2)\n",
    "    return observation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self,env,epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.num_observations = env.observation_space.n\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        # random action probabilities\n",
    "        self.action_probabilities = np.zeros((self.num_observations,self.num_actions),dtype=np.float32)\n",
    "        self.action_probabilities += np.random.rand(self.num_observations,self.num_actions)\n",
    "        normalize = np.sum(self.action_probabilities,axis=1)\n",
    "        normalize = normalize.reshape(self.num_observations,1)\n",
    "        self.action_probabilities /= normalize\n",
    "\n",
    "        self.uniform_action_probs = np.full((sef.num_observations,self.num_actions),1.0/self.num_actions,dtype=np.float32)\n",
    "        self.greedy_action_probabilities = self.uniform_action_probs #fail until first greedy update\n",
    "        \n",
    "    def sample(self,observation,test=False):\n",
    "        if test:\n",
    "            return np.random.choice(a=self.num_actions, p=self.greedy_action_probabilities[observation])\n",
    "        else:\n",
    "            return np.random.choice(a=self.num_actions, p=self.action_probabilities[observation])\n",
    "    \n",
    "    def upate(self,action_probabilities):\n",
    "        self.action_probabilities = action_probabilities\n",
    "        \n",
    "    def epsilon_greedy_update(self,Q_table,epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        greedy_actions = np.argmax(Q_table,axis=1)\n",
    "        self.greedy_action_probabilities = np.zeros((self.num_observations,self.num_actions),dtype=np.float32)\n",
    "        for i in range(self.num_observations):\n",
    "            self.greedy_action_probabilities[i][greedy_actions[i]] = 1.0\n",
    "\n",
    "        self.action_probabilities = (1.0-epsilon)*self.greedy_action_probabilities + epsilon * self.uniform_action_probabilities    \n",
    "        return self.action_probabilities\n",
    "    \n",
    "    def epsilon_greedy_update(self,epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_probabilities = (1.0-epsilon)*self.greedy_action_probabilities + epsilon * self.uniform_action_probabilities\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q(object):\n",
    "    def __init__(self,env,Q_table):\n",
    "        self.env = env\n",
    "        self.num_observations = env.observation_space.n\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        self.alpha = 0.1\n",
    "        self.gamma = .99\n",
    "        self.Q_table = np.zeros((self.num_observations,self.num_actions),dtype=np.float32)\n",
    "        if Q_table:\n",
    "            self.Q_table = Q_table\n",
    "    \n",
    "    def naive_empirical_update(self,policy, MC_batch_size):\n",
    "        action_probabilities = policy.action_probabilities\n",
    "        Q = np.zeros([env.observation_space.n,env.action_space.n],dtype=np.float32)\n",
    "        for observation_index in range(self.num_observations):\n",
    "            for action_index in range(self.num_actions):\n",
    "                sum = 0.0\n",
    "                for _ in range(MC_batch_size):\n",
    "                    future_returns = 0.0\n",
    "                    env.reset()\n",
    "                    observation = FLNotSlippery_set_observation(env,observation_index)\n",
    "                    observation, reward, done, _ = env.step(action_index)\n",
    "                    future_returns += reward\n",
    "                    for _ in range(max_steps):\n",
    "                        sample_action = policy.sample(observation)\n",
    "                        observation, reward, done, _ = env.step(sample_action)\n",
    "                        future_returns += self.gamma*reward\n",
    "                        if done:\n",
    "                            break\n",
    "                    sum += future_returns\n",
    "                sum /= MC_batch_size                \n",
    "                Q[observation_index][action_index]\n",
    "        return Q\n",
    "                \n",
    "    def SARSA_update(self,policy):\n",
    "        \n",
    "    def Q_optimal_update(self,sample_batch):\n",
    "        if dueling is not True:\n",
    "            Q_target_network_2 = Q_target_network\n",
    "        self.Q_targets = rewards + (gamma**nstep) + Q_target_network[state_next][np.argmax(Q_target_network_2[state_next])]\n",
    "        Q += alpha*(Q_targets-self.Q_table)\n",
    "    \n",
    "    def Q_update(self,policy):\n",
    "        for i in range(batch_size)\n",
    "            \n",
    "            self.Q_table[state][action] = alpha*(Q_targets - self.Q_table[state][action])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self,max_size):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.next_idx = 0\n",
    "    def add(self,state,action,reward,state_next,done):\n",
    "        data = (state, action, reward, state_next, done) # generalize to n steps\n",
    "        if next_idx >= len(self.buffer):\n",
    "            self.buffer.append(data)\n",
    "        else:\n",
    "            self.buffer[next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.max_size        \n",
    "    def sample(self,batch_size):\n",
    "        idxes = np.random.randint(0,len(self.buffer)-1,[batch_size])\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_next = []\n",
    "        dones = []\n",
    "        for idx in range(batch_size):\n",
    "            state, action, reward, state_next, done = self.buffer[idxes[idx]]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            states_next.append(state_next)\n",
    "            dones.append(done)\n",
    "        return np.array(states), np.array(actions),np.array(rewards),np.array(states_next),np.array(dones)\n",
    "    def clear(self):\n",
    "        self.buffer = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataCabinetRLseries",
   "language": "python",
   "name": "datacabinetrlseries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
